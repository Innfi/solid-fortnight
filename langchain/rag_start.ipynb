{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1ea292a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Now you can access them\n",
    "api_key = os.getenv('OPENAI_API_KEY')\n",
    "\n",
    "model = OpenAIEmbeddings(model='text-embedding-3-small', openai_api_key=api_key)\n",
    "embeddings = model.embed_documents([\n",
    "  'hi there!',\n",
    "  'oh, hello!',\n",
    "])\n",
    "\n",
    "print(embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2107f7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup PGVector and SQLRecordManager for RAG indexing\n",
    "import uuid\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Get database connection URL\n",
    "pgvector_url = os.getenv('PGVECTOR_URL')\n",
    "\n",
    "if not pgvector_url:\n",
    "    print(\"Warning: PGVECTOR_URL not found in environment variables\")\n",
    "    print(\"Please add PGVECTOR_URL=postgresql://user:password@localhost:5432/dbname to your .env file\")\n",
    "else:\n",
    "    print(f\"PGVector URL configured: {pgvector_url.split('@')[0]}@***\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32c532bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries for RAG indexing\n",
    "from langchain_community.indexes import *\n",
    "from langchain_core.documents import Document\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "import psycopg2\n",
    "from psycopg2.extensions import ISOLATION_LEVEL_AUTOCOMMIT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a732ca35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize embeddings model\n",
    "embeddings = OpenAIEmbeddings(\n",
    "    model='text-embedding-3-small',\n",
    "    openai_api_key=os.getenv('OPENAI_API_KEY')\n",
    ")\n",
    "\n",
    "# Define collection name for the vector store\n",
    "COLLECTION_NAME = \"rag_documents\"\n",
    "namespace = f\"pgvector/{COLLECTION_NAME}\"\n",
    "\n",
    "print(f\"Initializing embeddings model: text-embedding-3-small\")\n",
    "print(f\"Collection name: {COLLECTION_NAME}\")\n",
    "print(f\"Namespace: {namespace}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85b1d7fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_postgres import PGEngine, PGVectorStore\n",
    "\n",
    "engine = PGEngine.from_connection_string(pgvector_url)\n",
    "\n",
    "# Create PGVector store instance\n",
    "try:\n",
    "    vectorstore = PGVectorStore(\n",
    "        embeddings=embeddings,\n",
    "        collection_name=COLLECTION_NAME,\n",
    "        connection=pgvector_url,\n",
    "        use_jsonb=True,\n",
    "    )\n",
    "    print(\"‚úÖ PGVector store created successfully\")\n",
    "    \n",
    "    # Test connection\n",
    "    print(f\"Vector store collection: {vectorstore.collection_name}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error creating PGVector store: {e}\")\n",
    "    print(\"Make sure PostgreSQL with pgvector extension is running and accessible\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20068fed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "133fb505",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create SQLRecordManager instance for tracking indexed documents\n",
    "try:\n",
    "    record_manager = SQLRecordManager(\n",
    "        namespace=namespace,\n",
    "        db_url=pgvector_url\n",
    "    )\n",
    "    \n",
    "    # Create the schema for record manager (run this once)\n",
    "    record_manager.create_schema()\n",
    "    print(\"‚úÖ SQLRecordManager created successfully\")\n",
    "    print(f\"Record manager namespace: {record_manager.namespace}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error creating SQLRecordManager: {e}\")\n",
    "    print(\"This might be normal if the schema already exists\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeb4fbf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample documents for indexing\n",
    "sample_documents = [\n",
    "    Document(\n",
    "        page_content=\"\"\"\n",
    "        Artificial Intelligence (AI) is a broad field of computer science focused on creating \n",
    "        intelligent machines that can perform tasks typically requiring human intelligence. \n",
    "        AI encompasses various subfields including machine learning, natural language processing, \n",
    "        computer vision, and robotics. Modern AI systems use neural networks and deep learning \n",
    "        to achieve remarkable performance in tasks like image recognition, language translation, \n",
    "        and decision-making.\n",
    "        \"\"\",\n",
    "        metadata={\n",
    "            \"source\": \"ai_overview.txt\",\n",
    "            \"topic\": \"artificial_intelligence\",\n",
    "            \"category\": \"technology\",\n",
    "            \"author\": \"AI Research Team\",\n",
    "            \"date\": \"2024-01-15\"\n",
    "        }\n",
    "    ),\n",
    "    \n",
    "    Document(\n",
    "        page_content=\"\"\"\n",
    "        Machine Learning is a subset of artificial intelligence that focuses on algorithms \n",
    "        and statistical models that enable computer systems to improve their performance \n",
    "        on a specific task through experience. There are three main types of machine learning: \n",
    "        supervised learning (learning with labeled data), unsupervised learning (finding \n",
    "        patterns in unlabeled data), and reinforcement learning (learning through interaction \n",
    "        with an environment). Popular algorithms include linear regression, decision trees, \n",
    "        neural networks, and support vector machines.\n",
    "        \"\"\",\n",
    "        metadata={\n",
    "            \"source\": \"ml_basics.txt\",\n",
    "            \"topic\": \"machine_learning\",\n",
    "            \"category\": \"technology\",\n",
    "            \"author\": \"Data Science Team\",\n",
    "            \"date\": \"2024-01-20\"\n",
    "        }\n",
    "    ),\n",
    "    \n",
    "    Document(\n",
    "        page_content=\"\"\"\n",
    "        Natural Language Processing (NLP) is a field of AI that focuses on the interaction \n",
    "        between computers and human language. NLP combines computational linguistics with \n",
    "        machine learning and deep learning to help computers understand, interpret, and \n",
    "        generate human language in a valuable way. Common NLP tasks include sentiment analysis, \n",
    "        named entity recognition, machine translation, text summarization, and question answering. \n",
    "        Modern NLP heavily relies on transformer architectures like BERT, GPT, and T5.\n",
    "        \"\"\",\n",
    "        metadata={\n",
    "            \"source\": \"nlp_guide.txt\",\n",
    "            \"topic\": \"natural_language_processing\",\n",
    "            \"category\": \"technology\",\n",
    "            \"author\": \"NLP Research Lab\",\n",
    "            \"date\": \"2024-01-25\"\n",
    "        }\n",
    "    ),\n",
    "    \n",
    "    Document(\n",
    "        page_content=\"\"\"\n",
    "        Vector databases are specialized databases designed to store and efficiently search \n",
    "        high-dimensional vector embeddings. They are essential for modern AI applications \n",
    "        like semantic search, recommendation systems, and retrieval-augmented generation (RAG). \n",
    "        Vector databases use advanced indexing techniques like HNSW (Hierarchical Navigable \n",
    "        Small World) or IVF (Inverted File) to enable fast similarity searches across millions \n",
    "        or billions of vectors. Popular vector databases include Pinecone, Weaviate, Qdrant, \n",
    "        and PostgreSQL with pgvector extension.\n",
    "        \"\"\",\n",
    "        metadata={\n",
    "            \"source\": \"vector_db_intro.txt\",\n",
    "            \"topic\": \"vector_databases\",\n",
    "            \"category\": \"database\",\n",
    "            \"author\": \"Database Team\",\n",
    "            \"date\": \"2024-02-01\"\n",
    "        }\n",
    "    ),\n",
    "    \n",
    "    Document(\n",
    "        page_content=\"\"\"\n",
    "        Retrieval-Augmented Generation (RAG) is a powerful technique that combines \n",
    "        information retrieval with text generation to create more accurate and contextual \n",
    "        responses. RAG works by first retrieving relevant documents from a knowledge base \n",
    "        using semantic search, then using these documents as context for a large language \n",
    "        model to generate responses. This approach helps reduce hallucinations and provides \n",
    "        more factual, up-to-date information. RAG systems typically consist of a vector \n",
    "        database for document storage, an embedding model for creating vector representations, \n",
    "        and a language model for generation.\n",
    "        \"\"\",\n",
    "        metadata={\n",
    "            \"source\": \"rag_explained.txt\",\n",
    "            \"topic\": \"retrieval_augmented_generation\",\n",
    "            \"category\": \"ai_techniques\",\n",
    "            \"author\": \"AI Applications Team\",\n",
    "            \"date\": \"2024-02-05\"\n",
    "        }\n",
    "    )\n",
    "]\n",
    "\n",
    "print(f\"Created {len(sample_documents)} sample documents for indexing\")\n",
    "print(\"Sample topics:\", [doc.metadata[\"topic\"] for doc in sample_documents])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "524fc3ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Split documents into smaller chunks for better retrieval\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=500,\n",
    "    chunk_overlap=50,\n",
    "    length_function=len,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
    ")\n",
    "\n",
    "# Split the documents\n",
    "split_documents = []\n",
    "for doc in sample_documents:\n",
    "    chunks = text_splitter.split_documents([doc])\n",
    "    \n",
    "    # Add chunk information to metadata\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        chunk.metadata.update({\n",
    "            \"chunk_id\": f\"{doc.metadata['source']}_chunk_{i}\",\n",
    "            \"chunk_index\": i,\n",
    "            \"total_chunks\": len(chunks)\n",
    "        })\n",
    "    \n",
    "    split_documents.extend(chunks)\n",
    "\n",
    "print(f\"Original documents: {len(sample_documents)}\")\n",
    "print(f\"After splitting: {len(split_documents)} chunks\")\n",
    "print(f\"Average chunk size: {sum(len(doc.page_content) for doc in split_documents) // len(split_documents)} characters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77417db3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Index documents using the record manager for deduplication and tracking\n",
    "try:\n",
    "    print(\"üöÄ Starting document indexing...\")\n",
    "    \n",
    "    # Use the index function with record manager for smart indexing\n",
    "    # This will handle deduplication and track what's been indexed\n",
    "    result = index(\n",
    "        docs_source=split_documents,\n",
    "        record_manager=record_manager,\n",
    "        vector_store=vectorstore,\n",
    "        cleanup=\"incremental\",  # Only add new/changed documents\n",
    "        source_id_key=\"chunk_id\",  # Use chunk_id as unique identifier\n",
    "    )\n",
    "    \n",
    "    print(\"‚úÖ Document indexing completed!\")\n",
    "    print(f\"Documents added: {result['num_added']}\")\n",
    "    print(f\"Documents updated: {result['num_updated']}\")\n",
    "    print(f\"Documents skipped: {result['num_skipped']}\")\n",
    "    print(f\"Documents deleted: {result['num_deleted']}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error during indexing: {e}\")\n",
    "    print(\"Make sure PGVector and SQLRecordManager are properly configured\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71f4f12e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the vector store with a similarity search\n",
    "try:\n",
    "    query = \"What is machine learning and how does it work?\"\n",
    "    print(f\"üîç Testing similarity search with query: '{query}'\")\n",
    "    \n",
    "    # Perform similarity search\n",
    "    results = vectorstore.similarity_search(\n",
    "        query=query,\n",
    "        k=3  # Return top 3 most similar documents\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nüìã Found {len(results)} similar documents:\")\n",
    "    \n",
    "    for i, doc in enumerate(results, 1):\n",
    "        print(f\"\\n--- Result {i} ---\")\n",
    "        print(f\"Source: {doc.metadata.get('source', 'Unknown')}\")\n",
    "        print(f\"Topic: {doc.metadata.get('topic', 'Unknown')}\")\n",
    "        print(f\"Chunk ID: {doc.metadata.get('chunk_id', 'Unknown')}\")\n",
    "        print(f\"Content preview: {doc.page_content[:200]}...\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error during similarity search: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31bd1172",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify indexing status and get statistics\n",
    "try:\n",
    "    print(\"üìä Vector Store Statistics:\")\n",
    "    print(\"-\" * 30)\n",
    "    \n",
    "    # Get total document count\n",
    "    all_docs = vectorstore.similarity_search(\"\", k=100)  # Get all docs (up to 100)\n",
    "    print(f\"Total indexed documents: {len(all_docs)}\")\n",
    "    \n",
    "    # Show unique topics\n",
    "    topics = set(doc.metadata.get('topic', 'unknown') for doc in all_docs)\n",
    "    print(f\"Unique topics: {len(topics)}\")\n",
    "    print(f\"Topics: {', '.join(sorted(topics))}\")\n",
    "    \n",
    "    # Show sources\n",
    "    sources = set(doc.metadata.get('source', 'unknown') for doc in all_docs)\n",
    "    print(f\"Unique sources: {len(sources)}\")\n",
    "    print(f\"Sources: {', '.join(sorted(sources))}\")\n",
    "    \n",
    "    # Record manager status\n",
    "    print(f\"\\nRecord Manager Namespace: {record_manager.namespace}\")\n",
    "    \n",
    "    print(\"\\n‚úÖ RAG indexing setup complete!\")\n",
    "    print(\"You can now use the vector store for retrieval-augmented generation\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error getting statistics: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0021f29",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragatouille import RAGPretrainedModel\n",
    "\n",
    "RAG = RAGPretrainedModel.from_pretrained(\"colbert-ir/colbert2.0\")\n",
    "import requests\n",
    "\n",
    "def get_wikipedia_summary(title: str):\n",
    "  URL = \"https://en.wikipedia.org/w/api.php\"\n",
    "  PARAMS = {\n",
    "      \"action\": \"query\",\n",
    "      \"format\": \"json\",\n",
    "      \"prop\": \"extracts\",\n",
    "      \"exintro\": True,\n",
    "      \"explaintext\": True,\n",
    "      \"titles\": title,\n",
    "  }\n",
    "\n",
    "  headers = {\n",
    "      \"User-Agent\": \"RAGatouille_tutorial/0.0.1\"\n",
    "  }\n",
    "  response = requests.get(URL, params=PARAMS, headers=headers)\n",
    "  data = response.json()\n",
    "\n",
    "  page = next(iter(data['query']['pages'].values()))\n",
    "  return page.get('extract', 'No summary available.')\n",
    "\n",
    "full_document = get_wikipedia_summary(\"Artificial intelligence\")\n",
    "\n",
    "RAG.index(\n",
    "  collection=[full_document],\n",
    "  index_name=\"wiki_ai_test\",\n",
    "  max_document_length=180,\n",
    "  split_documents=True,\n",
    ")\n",
    "\n",
    "results = RAG.search(query=\"what is the core value of large language models?\", index_name=\"wiki_ai_test\", k=3)\n",
    "results\n",
    "\n",
    "#retriever = RAG.as_langchain_retriever(index_name=\"wiki_ai_test\", k=3)\n",
    "#retriever.invoke(\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
